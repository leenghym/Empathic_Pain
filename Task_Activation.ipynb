{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c9c416",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nilearn nibabel pandas matplotlib\n",
    "%pip install --upgrade nilearn\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nilearn\n",
    "import glob\n",
    "import re\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn.image import load_img\n",
    "from nilearn.plotting import plot_epi, plot_design_matrix, plot_contrast_matrix, plot_stat_map\n",
    "from nilearn.glm.first_level import FirstLevelModel\n",
    "from nilearn.glm.contrasts import compute_contrast\n",
    "from nilearn.glm.second_level import SecondLevelModel\n",
    "from nilearn.plotting import plot_stat_map\n",
    "from nilearn.reporting import get_clusters_table\n",
    "from nilearn.image import coord_transform\n",
    "from nilearn.image import resample_to_img, load_img\n",
    "from nilearn.input_data import NiftiLabelsMasker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99be8030",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nilearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d2167b",
   "metadata": {},
   "source": [
    "### Download/load brain atlas (for anatomical labelling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9019192d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.datasets import fetch_atlas_harvard_oxford\n",
    "# Fetch the Harvard-Oxford atlas\n",
    "atlas = fetch_atlas_harvard_oxford('cort-maxprob-thr25-2mm')\n",
    "atlas_img = load_img(atlas.maps)\n",
    "atlas_labels = list(atlas.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209f7f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Paths\n",
    "func_dir = '/Users/onjolikrywiak/Desktop/BrainHack/sub-001/func'\n",
    "reg_dir = '/Users/onjolikrywiak/Desktop/BrainHack/events/sub-001/regressors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f4351c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Grab only the Lin2009cAsym bold files\n",
    "pattern   = os.path.join(func_dir, '*MNI152NLin2009cAsym*_bold.nii*')\n",
    "nii_files = sorted(glob.glob(pattern))\n",
    "print(\"Found bold files:\", nii_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a101f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Loop & pair each with its confounds\n",
    "for nii_path in nii_files:\n",
    "    basename = os.path.basename(nii_path)\n",
    "    \n",
    "    # Extract subj_task and run number, e.g. \"sub-001_task-empathy\" + \"run-01\"\n",
    "    m = re.match(r'(sub-[^_]+_task-[^_]+).*?(run-\\d+)', basename)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Can't parse subject/task/run from {basename}\")\n",
    "    subj_task = m.group(1)\n",
    "    run       = m.group(2)\n",
    "    \n",
    "    # Build confounds filename: sub-001_task-empathy_run-01_confounds.txt\n",
    "    conf_name = f\"{subj_task}_{run}_confounds.txt\"\n",
    "    conf_path = os.path.join(reg_dir, conf_name)\n",
    "    if not os.path.exists(conf_path):\n",
    "        raise FileNotFoundError(f\"Confounds file not found: {conf_path}\")\n",
    "    \n",
    "    # Load the bold image\n",
    "    img  = nib.load(nii_path)\n",
    "    data = img.get_fdata()\n",
    "    print(f\"{basename} â†’ image shape {data.shape}\")\n",
    "    \n",
    "    # Load the confounds\n",
    "    # adjust sep if whitespace-delimited; here we assume tab-delimited\n",
    "    conf_df = pd.read_csv(conf_path, sep='\\t', header=None)\n",
    "    print(f\"{conf_name} â†’ confounds shape {conf_df.shape}\")\n",
    "    \n",
    "    # Assign to run-specific variables\n",
    "    if run == 'run-01':\n",
    "        img1, data1, path1, conf1 = img, data, nii_path, conf_df\n",
    "    elif run == 'run-02':\n",
    "        img2, data2, path2, conf2 = img, data, nii_path, conf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ba4a4d",
   "metadata": {},
   "source": [
    "### Trim 3 timepoints from the beginning and end of BOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d42ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim 3 timepoints from the beginning and end of BOLD\n",
    "trim = 3\n",
    "if data.shape[-1] > 2 * trim:\n",
    "    data = data[..., trim:-trim]\n",
    "    print(f\"Trimmed BOLD shape: {data.shape}\")\n",
    "else:\n",
    "    raise ValueError(\"Not enough timepoints to trim 3 from each end.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1e4623",
   "metadata": {},
   "source": [
    "### Check to see if BOLD and confounds are the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722dd98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"BOLD timepoints: {data.shape[-1]}\")\n",
    "print(f\"Confounds timepoints: {conf_df.shape[0]}\")\n",
    "\n",
    "if data.shape[-1] == conf_df.shape[0]:\n",
    "    print(\"BOLD and confounds are aligned!\")\n",
    "else:\n",
    "    print(\"Warning: BOLD and confounds are NOT aligned!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcda026d",
   "metadata": {},
   "source": [
    "# First Level Analysis (GLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6f44c6",
   "metadata": {},
   "source": [
    "## Generate a Z-Map for One Trial Type (Single Subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c323a0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Parse your custom events file\n",
    "events_txt = '/Users/onjolikrywiak/Desktop/BrainHack/events/sub-001/regressors/sub-001_task-empathy_timing-selfpain.txt'\n",
    "with open(events_txt) as f:\n",
    "    line = f.readline().strip()\n",
    "pairs = line.split('\\t')\n",
    "\n",
    "# Build events DataFrame\n",
    "onsets = []\n",
    "durations = []\n",
    "for pair in pairs:\n",
    "    onset, duration = pair.split(':')\n",
    "    onsets.append(float(onset))\n",
    "    durations.append(float(duration))\n",
    "events = pd.DataFrame({\n",
    "    'onset': onsets,\n",
    "    'duration': durations,\n",
    "    'trial_type': 'selfpain'  # or another label if needed\n",
    "})\n",
    "\n",
    "# 2. Fit the GLM (no confounds)\n",
    "fmri_glm = FirstLevelModel(\n",
    "    t_r=2.0,  # set your TR\n",
    "    noise_model='ar1',\n",
    "    standardize=True,\n",
    "    hrf_model='spm',\n",
    "    drift_model='cosine'\n",
    ")\n",
    "fmri_glm = fmri_glm.fit(img, events=events)\n",
    "\n",
    "# 3. Inspect design matrix columns\n",
    "print(\"Design matrix columns:\", fmri_glm.design_matrices_[0].columns.tolist())\n",
    "\n",
    "# 4. Define and compute the contrast\n",
    "contrast_name = 'selfpain'\n",
    "if contrast_name in fmri_glm.design_matrices_[0].columns:\n",
    "    contrast_vec = (fmri_glm.design_matrices_[0].columns == contrast_name).astype(int)\n",
    "    z_map = fmri_glm.compute_contrast(contrast_vec, output_type='z_score')\n",
    "\n",
    "    # 5. Plot the result\n",
    "    plot_stat_map(z_map, title=f'{contrast_name} > baseline', threshold=3.1)\n",
    "else:\n",
    "    print(f\"Contrast '{contrast_name}' not found in design matrix columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4938f232",
   "metadata": {},
   "source": [
    "## Generate Z-Maps for All Trial Types (Single Subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31da35d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your regressors folder\n",
    "reg_dir = '/Users/onjolikrywiak/Desktop/BrainHack/events/sub-001/regressors'\n",
    "\n",
    "# Find all timing files in the folder\n",
    "event_files = glob.glob(os.path.join(reg_dir, 'sub-001_task-empathy_timing-*.txt'))\n",
    "\n",
    "for events_txt in event_files:\n",
    "    # Extract trial type from filename\n",
    "    basename = os.path.basename(events_txt)\n",
    "    trial_type = basename.split('timing-')[1].replace('.txt', '')\n",
    "\n",
    "    # Parse the custom events file\n",
    "    with open(events_txt) as f:\n",
    "        line = f.readline().strip()\n",
    "    pairs = line.split('\\t')\n",
    "\n",
    "    # Build events DataFrame\n",
    "    onsets = []\n",
    "    durations = []\n",
    "    for pair in pairs:\n",
    "        onset, duration = pair.split(':')\n",
    "        onsets.append(float(onset))\n",
    "        durations.append(float(duration))\n",
    "    events = pd.DataFrame({\n",
    "        'onset': onsets,\n",
    "        'duration': durations,\n",
    "        'trial_type': trial_type\n",
    "    })\n",
    "\n",
    "    # Fit the GLM (no confounds)\n",
    "    fmri_glm = FirstLevelModel(\n",
    "        t_r=2.0,\n",
    "        noise_model='ar1',\n",
    "        standardize=True,\n",
    "        hrf_model='spm',\n",
    "        drift_model='cosine'\n",
    "    )\n",
    "    fmri_glm = fmri_glm.fit(img, events=events)\n",
    "\n",
    "    # Inspect design matrix columns\n",
    "    print(f\"Design matrix columns for {trial_type}:\", fmri_glm.design_matrices_[0].columns.tolist())\n",
    "\n",
    "    # Define and compute the contrast\n",
    "    contrast_name = trial_type\n",
    "    if contrast_name in fmri_glm.design_matrices_[0].columns:\n",
    "        contrast_vec = (fmri_glm.design_matrices_[0].columns == contrast_name).astype(int)\n",
    "        z_map = fmri_glm.compute_contrast(contrast_vec, output_type='z_score')\n",
    "\n",
    "        # Plot the result\n",
    "        plot_stat_map(z_map, title=f'{contrast_name} > baseline', threshold=3.1)\n",
    "    else:\n",
    "        print(f\"Contrast '{contrast_name}' not found in design matrix columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647568df",
   "metadata": {},
   "source": [
    "## Generate Z-Maps and Cluster Tables for All Subjects and Trial Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb37bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_types = [\"otherneutralanticipation\",\n",
    "               \"otherneutralcue\",\n",
    "               \"othernopain\",\n",
    "               \"othernopainrest\",\n",
    "               \"otherpain\",\n",
    "               \"otherpainanticipation\",\n",
    "               \"otherpainpaincue\",\n",
    "               \"otherpainrest\",\n",
    "               \"selfneutralanticipation\",\n",
    "               \"selfneutralcue\",\n",
    "               \"selfnopain\",\n",
    "               \"selfnopainrest\",\n",
    "               \"selfpain\",\n",
    "               \"selfpainpaincue\",\n",
    "               \"selfpainanticipation\",\n",
    "               \"selfpainrest\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b23231",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/Users/onjolikrywiak/Desktop/BrainHack/outputs'\n",
    "subjects = sorted({f.split('_')[0] for f in glob.glob(os.path.join(output_dir, 'sub-*_selfpain_zmap.nii.gz'))})\n",
    "\n",
    "for subj in subjects:\n",
    "    for trial_type in trial_types:\n",
    "        zmap_path = os.path.join(output_dir, f\"{subj}_{trial_type}_zmap.nii.gz\")\n",
    "        # If z-map does not exist, try to create it\n",
    "        if not os.path.exists(zmap_path):\n",
    "            # Try to find the functional image and events file\n",
    "            func_pattern = f\"/Users/onjolikrywiak/Desktop/BrainHack/{subj}/func/{subj}_*MNI152NLin2009cAsym*_bold.nii*\"\n",
    "            func_files = sorted(glob.glob(func_pattern))\n",
    "            if not func_files:\n",
    "                print(f\"No functional file found for {subj}, skipping {trial_type}\")\n",
    "                continue\n",
    "            img = load_img(func_files[0])  # You may want to refine this selection\n",
    "            # Find events file\n",
    "            events_pattern = f\"/Users/onjolikrywiak/Desktop/BrainHack/events/{subj}/regressors/{subj}_task-empathy_timing-{trial_type}.txt\"\n",
    "            if not os.path.exists(events_pattern):\n",
    "                print(f\"No events file found for {subj} {trial_type}, skipping\")\n",
    "                continue\n",
    "            with open(events_pattern) as f:\n",
    "                line = f.readline().strip()\n",
    "            pairs = line.split('\\t')\n",
    "            onsets, durations = [], []\n",
    "            for pair in pairs:\n",
    "                onset, duration = pair.split(':')\n",
    "                onsets.append(float(onset))\n",
    "                durations.append(float(duration))\n",
    "            events = pd.DataFrame({\n",
    "                'onset': onsets,\n",
    "                'duration': durations,\n",
    "                'trial_type': trial_type\n",
    "            })\n",
    "            # Fit GLM and compute z-map\n",
    "            fmri_glm = FirstLevelModel(\n",
    "                t_r=2.0,\n",
    "                noise_model='ar1',\n",
    "                standardize=True,\n",
    "                hrf_model='spm',\n",
    "                drift_model='cosine'\n",
    "            )\n",
    "            fmri_glm = fmri_glm.fit(img, events=events)\n",
    "            if trial_type in fmri_glm.design_matrices_[0].columns:\n",
    "                contrast_vec = (fmri_glm.design_matrices_[0].columns == trial_type).astype(int)\n",
    "                z_map = fmri_glm.compute_contrast(contrast_vec, output_type='z_score')\n",
    "                z_map.to_filename(zmap_path)\n",
    "                print(f\"Created z-map: {zmap_path}\")\n",
    "            else:\n",
    "                print(f\"Contrast {trial_type} not found for {subj}, skipping\")\n",
    "                continue\n",
    "        else:\n",
    "            z_map = load_img(zmap_path)\n",
    "        # Generate cluster table\n",
    "        table = get_clusters_table(z_map, stat_threshold=3.1)\n",
    "        # Resample atlas to match z-map\n",
    "        resampled_atlas = resample_to_img(atlas_img, z_map, interpolation='nearest')\n",
    "        def get_label_from_coords(coord):\n",
    "            voxel_coords = np.round(coord_transform(*coord, np.linalg.inv(resampled_atlas.affine))).astype(int)\n",
    "            data = resampled_atlas.get_fdata()\n",
    "            try:\n",
    "                label_index = int(data[tuple(voxel_coords)])\n",
    "                if label_index > 0 and label_index < len(atlas_labels):\n",
    "                    return atlas_labels[label_index]\n",
    "                else:\n",
    "                    return 'Unknown'\n",
    "            except Exception:\n",
    "                return 'Unknown'\n",
    "        if not table.empty and all(col in table.columns for col in ['X', 'Y', 'Z']):\n",
    "            table['peak_coord'] = list(zip(table['X'], table['Y'], table['Z']))\n",
    "            table['Region_Label'] = table['peak_coord'].apply(get_label_from_coords)\n",
    "        else:\n",
    "            table['peak_coord'] = [None] * len(table)\n",
    "            table['Region_Label'] = [None] * len(table)\n",
    "        # Save subject-level cluster table\n",
    "        csv_path = os.path.join(output_dir, f\"{subj}_{trial_type}_clusters.csv\")\n",
    "        table.to_csv(csv_path, index=False)\n",
    "        print(f\"Saved cluster table: {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f0b8dd",
   "metadata": {},
   "source": [
    "## Summary Table for a Region of Interest (All Subjects and Trial Types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95477e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_of_interest = \"Insular Cortex\"  # <-- change this to your region\n",
    "output_dir = '/Users/onjolikrywiak/Desktop/BrainHack/outputs'\n",
    "\n",
    "summary_rows = []\n",
    "\n",
    "for csv_file in glob.glob(os.path.join(output_dir, 'sub-*_*.csv')):\n",
    "    # Extract subject and trial_type from filename\n",
    "    m = re.match(r'(sub-\\d+)_([a-zA-Z0-9]+)_clusters\\.csv', os.path.basename(csv_file))\n",
    "    if not m:\n",
    "        continue\n",
    "    subj = m.group(1)\n",
    "    trial_type = m.group(2)\n",
    "    table = pd.read_csv(csv_file)\n",
    "    region_rows = table[table['Region_Label'] == region_of_interest]\n",
    "    for _, row in region_rows.iterrows():\n",
    "        summary_rows.append({\n",
    "            'Subject': subj,  # Now always just sub-001, sub-002, etc.\n",
    "            'TrialType': trial_type,\n",
    "            'X': row.get('X', None),\n",
    "            'Y': row.get('Y', None),\n",
    "            'Z': row.get('Z', None),\n",
    "            'PeakStat': row.get('Peak Stat', None),\n",
    "            'ClusterSize': row.get('Cluster Size (mm3)', None)\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_df = summary_df.sort_values(by=[\"Subject\", \"TrialType\"]).reset_index(drop=True)\n",
    "print(summary_df)\n",
    "\n",
    "summary_df.to_csv(os.path.join(output_dir, f\"{region_of_interest.replace(' ', '_')}_summary.csv\"), index=False)\n",
    "print(f\"Saved summary to {region_of_interest.replace(' ', '_')}_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecc9cce",
   "metadata": {},
   "source": [
    "# Second-level (group) analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74446dac",
   "metadata": {},
   "source": [
    "## Group Z-maps and cluster tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eb3ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for trial_type in trial_types:\n",
    "    zmap_files = sorted(glob.glob(f'/Users/onjolikrywiak/Desktop/BrainHack/outputs/sub-*_{trial_type}_zmap.nii.gz'))\n",
    "    if not zmap_files:\n",
    "        print(f\"No zmaps found for {trial_type}, skipping.\")\n",
    "        continue\n",
    "    zmap_imgs = [load_img(f) for f in zmap_files]\n",
    "    print(f\"Found zmaps for {trial_type}:\", zmap_files)\n",
    "\n",
    "    # Second-level analysis\n",
    "    n_subs = len(zmap_imgs)\n",
    "    design_matrix = pd.DataFrame([1] * n_subs, columns=['intercept'])\n",
    "    second_level_model = SecondLevelModel()\n",
    "    second_level_model = second_level_model.fit(zmap_imgs, design_matrix=design_matrix)\n",
    "    zmap_group = second_level_model.compute_contrast('intercept', output_type='z_score')\n",
    "\n",
    "    from nilearn.glm import threshold_stats_img\n",
    "    zmap_group = second_level_model.compute_contrast('intercept', output_type='z_score')\n",
    "    \n",
    "    # FDR correction ## Activation is too weak to survive FDR or FWE correction\n",
    "    #thresholded_map, threshold = threshold_stats_img(\n",
    "    #    zmap_group, alpha=0.1, height_control='fdr'\n",
    "    #    )\n",
    "    #print(f\"FDR threshold used: {threshold}\")\n",
    "\n",
    "    # Plot and save\n",
    "    plot_stat_map(\n",
    "        zmap_group,\n",
    "        threshold=3.1,\n",
    "        title=f'Group-level {trial_type} z-map',\n",
    "        output_file=f'/Users/onjolikrywiak/Desktop/BrainHack/outputs/group_{trial_type}_zmap.png'\n",
    "    )\n",
    "    zmap_group.to_filename(f'/Users/onjolikrywiak/Desktop/BrainHack/outputs/group_{trial_type}_zmap.nii.gz')\n",
    "    print(f\"Saved group z-map for {trial_type}\")\n",
    "\n",
    "    # Get and save clusters table\n",
    "    table = get_clusters_table(zmap_group, stat_threshold=3.1)\n",
    "    \n",
    "    # Resample atlas to match your group z-map\n",
    "    resampled_atlas = resample_to_img(atlas_img, zmap_group, interpolation='nearest')\n",
    "    \n",
    "    def get_label_from_coords(coord):\n",
    "        from nilearn.image import coord_transform\n",
    "        import numpy as np\n",
    "        # Convert MNI coordinates to voxel indices\n",
    "        voxel_coords = np.round(coord_transform(*coord, np.linalg.inv(resampled_atlas.affine))).astype(int)\n",
    "        data = resampled_atlas.get_fdata()\n",
    "        try:\n",
    "            label_index = int(data[tuple(voxel_coords)])\n",
    "            if label_index > 0 and label_index < len(atlas_labels):\n",
    "                return atlas_labels[label_index]\n",
    "            else:\n",
    "                return 'Unknown'\n",
    "        except Exception:\n",
    "            return 'Unknown'\n",
    "        \n",
    "    # Add anatomical labels to the cluster table\n",
    "    if not table.empty and all(col in table.columns for col in ['X', 'Y', 'Z']):\n",
    "        table['peak_coord'] = list(zip(table['X'], table['Y'], table['Z']))\n",
    "        table['Region_Label'] = table['peak_coord'].apply(get_label_from_coords)\n",
    "    else:\n",
    "        table['peak_coord'] = [None] * len(table)\n",
    "        table['Region_Label'] = [None] * len(table)\n",
    "\n",
    "    # Save table to CSV\n",
    "    csv_path = f'/Users/onjolikrywiak/Desktop/BrainHack/outputs/group_{trial_type}_clusters.csv'\n",
    "    table.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved clusters table for {trial_type} to {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362a5be1",
   "metadata": {},
   "source": [
    "## How many times do anatomical regions appear across all trial types?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56538dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from collections import Counter\n",
    "\n",
    "summary = {}\n",
    "\n",
    "# Loop through all cluster tables\n",
    "for trial_type in trial_types:\n",
    "    csv_path = f'/Users/onjolikrywiak/Desktop/BrainHack/outputs/group_{trial_type}_clusters.csv'\n",
    "    try:\n",
    "        table = pd.read_csv(csv_path)\n",
    "        # Count occurrences of each region label\n",
    "        region_counts = Counter(table['Region_Label'].dropna())\n",
    "        summary[trial_type] = dict(region_counts)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not process {csv_path}: {e}\")\n",
    "\n",
    "# Print summary for each trial type\n",
    "for trial_type, regions in summary.items():\n",
    "    print(f\"\\n{trial_type}:\")\n",
    "    for region, count in regions.items():\n",
    "        print(f\"  {region}: {count} clusters\")\n",
    "\n",
    "# Optionally, save the summary to a CSV\n",
    "summary_df = pd.DataFrame(summary).fillna(0).astype(int)\n",
    "summary_df.to_csv('/Users/onjolikrywiak/Desktop/BrainHack/outputs/region_summary.csv')\n",
    "print(\"Saved region summary to outputs/region_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e62a566",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
